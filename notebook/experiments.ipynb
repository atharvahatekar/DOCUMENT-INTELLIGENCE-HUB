{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3c228dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "67044244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7bca2f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user is asking, \"What is the capital of France?\" Let me think. I remember that France is a country in Europe, and the capital is a major city there. From what I've studied before, the capital of France is Paris. But let me double-check to be sure. Sometimes countries can have different capitals due to historical reasons or something.\n",
      "\n",
      "Wait, Paris is known for the Eiffel Tower, the Louvre Museum, and it's a very famous city. I don't think any other city in France is considered the capital. Maybe Lyon or Marseille? No, those are big cities too, but I'm pretty sure Paris is the capital. Let me recall if there's any confusion here. I think the answer is definitely Paris. Yeah, that's right. So the capital of France is Paris. I don't see any reason to doubt that. Unless there's some recent political change, but I haven't heard of anything like that. So the answer should be Paris.\n",
      "</think>\n",
      "\n",
      "The capital of France is **Paris**. It is one of the most iconic cities in the world, renowned for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. Paris has served as the political, cultural, and economic center of France for centuries. \n",
      "\n",
      "**Answer:** Paris.\n"
     ]
    }
   ],
   "source": [
    "llm_model = ChatGroq(\n",
    "    model=\"qwen/qwen3-32b\"\n",
    ")\n",
    "\n",
    "response = llm_model.invoke(\"What is the capital of France?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e26aedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_model=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "19acecd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_vector = embedding_model.embed_query(\"What is the capital of France?\")\n",
    "len(embeded_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b292301",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824fdf9",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f444a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7a563c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cc23c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pdf_file_path = os.path.join(os.getcwd(), \"data\", \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "390c44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfLoader = PyPDFLoader(pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3662e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pdfLoader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bf61bffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "378e8825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, calledLlama 2-Chat, are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nour human evaluations for helpfulness and safety, may be a suitable substitute for closed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements ofLlama 2-Chatin order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.\\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "896b1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This are experimental values, there is no deteministic way to split the text.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=150,\n",
    "                length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "992591b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "97e7ed9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49298019",
   "metadata": {},
   "source": [
    "### Storing data in Vector Database(Faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6167c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "33935fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#InMemory Storage\n",
    "vectorDB =FAISS.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "37f7059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_doc = vectorDB.similarity_search(\"Training of Llama 2-Chat\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dffbbb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Figure 4: Training ofLlama 2-Chat: This process begins with thepretraining of Llama 2using publicly\\navailable online sources. Following this, we create an initial version ofLlama 2-Chatthrough the application\\nof supervised fine-tuning. Subsequently, the model is iteratively refined using Reinforcement Learning\\nwith Human Feedback(RLHF) methodologies, specifically through rejection sampling and Proximal Policy\\nOptimization (PPO). Throughout the RLHF stage, the accumulation ofiterative reward modeling datain\\nparallel with model enhancements is crucial to ensure the reward models remain within distribution.\\n2 Pretraining\\nTocreatethenewfamilyof Llama 2models,webeganwiththepretrainingapproachdescribedinTouvronetal.\\n(2023), using an optimized auto-regressive transformer, but made several changes to improve performance.\\nSpecifically, we performed more robust data cleaning, updated our data mixes, trained on 40% more total'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_doc[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "63b471d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#object for retriver of relevant documents from vector database\n",
    "retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ec143a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='e3f0fdb3-f309-4b9d-895d-9a9a57f51237', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\2.My Data\\\\Course_LLMOPS Industry Ready Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic'),\n",
       " Document(id='1713ec3d-1488-406b-862e-a7c81c2cfcfc', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\2.My Data\\\\Course_LLMOPS Industry Ready Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 4, 'page_label': '5'}, page_content='Figure 4: Training ofLlama 2-Chat: This process begins with thepretraining of Llama 2using publicly\\navailable online sources. Following this, we create an initial version ofLlama 2-Chatthrough the application\\nof supervised fine-tuning. Subsequently, the model is iteratively refined using Reinforcement Learning\\nwith Human Feedback(RLHF) methodologies, specifically through rejection sampling and Proximal Policy\\nOptimization (PPO). Throughout the RLHF stage, the accumulation ofiterative reward modeling datain\\nparallel with model enhancements is crucial to ensure the reward models remain within distribution.\\n2 Pretraining\\nTocreatethenewfamilyof Llama 2models,webeganwiththepretrainingapproachdescribedinTouvronetal.\\n(2023), using an optimized auto-regressive transformer, but made several changes to improve performance.\\nSpecifically, we performed more robust data cleaning, updated our data mixes, trained on 40% more total'),\n",
       " Document(id='42614313-69bf-447a-bb3d-744b1a408d23', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\2.My Data\\\\Course_LLMOPS Industry Ready Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 35, 'page_label': '36'}, page_content='of 7 billion to 70 billion parameters. These models have demonstrated their competitiveness with existing\\nopen-source chat models, as well as competency that is equivalent to some proprietary models on evaluation\\nsets we examined, although they still lag behind other models like GPT-4. We meticulously elaborated on the\\nmethodsandtechniquesappliedinachievingourmodels,withaheavyemphasisontheiralignmentwiththe\\nprinciplesofhelpfulnessandsafety. Tocontributemoresignificantlytosocietyandfosterthepaceofresearch,\\nwe have responsibly opened access toLlama 2andLlama 2-Chat. As part of our ongoing commitment to\\ntransparency and safety, we plan to make further improvements toLlama 2-Chatin future work.\\n36'),\n",
       " Document(id='985f6ebd-3949-49e2-82e1-5e6f10f0824a', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\2.My Data\\\\Course_LLMOPS Industry Ready Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 2, 'page_label': '3'}, page_content='continue to improve the safety of those models, paving the way for more responsible development of LLMs.\\nWe also share novel observations we made during the development ofLlama 2andLlama 2-Chat, such as\\nthe emergence of tool usage and temporal organization of knowledge.\\n3'),\n",
       " Document(id='74e3a33b-83cf-4e9b-b656-975438f9cbbc', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\2.My Data\\\\Course_LLMOPS Industry Ready Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 15, 'page_label': '16'}, page_content='simply set the loss to 0 for all the tokens from the previous turns, including assistant messages.\\nFor the training instructions, we created a few synthetic constraints to sample from: Hobbies (“You enjoy\\ne.g. Tennis”), Language (“Speak in e.g. French”), or Public Figure (“Act as e.g. Napoleon”). To obtain the lists\\nof hobbies and public figures, we askedLlama 2-Chatto generate it, avoiding a mismatch between the\\ninstruction and model knowledge (e.g., asking the model to act as someone it had not encountered during\\ntraining). Tomaketheinstructionsmorecomplexanddiverse,weconstructthefinalinstructionbyrandomly\\ncombining the above constraints. When constructing the final system message for the training data, we also\\n16')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Training of Llama 2-Chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e5ad5",
   "metadata": {},
   "source": [
    "### Question: user question  \n",
    "### Context: based on the question retrieving the info from the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b47a67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below in 2 or 3 lines. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "89f21443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ffc44569",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8f04828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4f876489",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e71da4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom function to extract the page_content from the documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "25524803",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8a83e4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user is asking about the training of Llama 2-Chat. Let me look through the provided context to find the relevant information.\n",
      "\n",
      "First, the context mentions that Llama 2-Chat's training starts with pretraining Llama 2 using public online sources. Then they do supervised fine-tuning to create an initial version. After that, they use RLHF (Reinforcement Learning with Human Feedback) with rejection sampling and PPO. During RLHF, they accumulate reward model data in parallel with model improvements to keep the reward models in distribution. \n",
      "\n",
      "I need to make sure I cover all the steps: pretraining, supervised fine-tuning, then RLHF with the specific methods mentioned. Also, note the part about using synthetic constraints for training instructions like Hobbies, Language, Public Figures. They combined these to make instructions more complex. \n",
      "\n",
      "Wait, there's a part about the final learning rate and other hyperparameters, but maybe that's more detail than needed. The user just wants a summary of the training process. So I should stick to the main steps: pretraining, supervised fine-tuning, RLHF with rejection sampling and PPO. Also mention the data accumulation during RLHF. Avoid getting into the specific numbers unless necessary. \n",
      "\n",
      "The context also says they made the instructions complex by combining constraints. That's part of the training data preparation. Should I include that? The user's question is about the training process, so maybe yes. So the answer should be concise, covering the main stages: pretraining, supervised fine-tuning, RLHF with the mentioned techniques, and the synthetic constraints used in training data.\n",
      "</think>\n",
      "\n",
      "The training of Llama 2-Chat begins with pretraining Llama 2 on public online data, followed by supervised fine-tuning. It is then refined iteratively using Reinforcement Learning with Human Feedback (RLHF), incorporating rejection sampling and Proximal Policy Optimization (PPO). Synthetic constraints (e.g., hobbies, languages) are used to generate diverse training instructions, ensuring alignment with human preferences and model knowledge.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"tell  me about the Training of Llama 2-Chat?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0069779a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
